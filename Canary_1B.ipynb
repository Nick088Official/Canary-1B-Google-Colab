{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nick088Official/Canary-1B-Google-Colab/blob/main/Canary_1B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0iXOUTOHIEr"
      },
      "source": [
        "# Canary 1B Model from Nvidia ASR (Automatic Speech Recognition) Toolkit for Transcripting Audios\n",
        "\n",
        "Made by [Nick088](https://linktr.ee/Nick088)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "sH1u0Ntu-IC5"
      },
      "outputs": [],
      "source": [
        "#@title Install NeMo ASR Canary 1B Model\n",
        "#@markdown If you get a warning popup about restarting runtime, just click cancel, you don't have to restart it\n",
        "!pip install Cython\n",
        "!pip install git+https://github.com/NVIDIA/NeMo.git@0bb9e66a6d29b28e8831d1d1dd8a30310173ce46#egg=nemo_toolkit[all] # commit from main when canary buffered inference merged\n",
        "!pip install ffmpeg\n",
        "!pip install libsndfile1\n",
        "\n",
        "import json\n",
        "import librosa\n",
        "import os\n",
        "import soundfile as sf\n",
        "import tempfile\n",
        "import uuid\n",
        "import torch\n",
        "from IPython.display import clear_output\n",
        "\n",
        "from nemo.collections.asr.models import ASRModel\n",
        "from nemo.collections.asr.parts.utils.streaming_utils import FrameBatchMultiTaskAED\n",
        "from nemo.collections.asr.parts.utils.transcribe_utils import get_buffered_pred_feat_multitaskAED\n",
        "\n",
        "SAMPLE_RATE = 16000  # Hz\n",
        "\n",
        "model = ASRModel.from_pretrained(\"nvidia/canary-1b\")\n",
        "model.eval()\n",
        "\n",
        "# make sure beam size always 1 for consistency\n",
        "model.change_decoding_strategy(None)\n",
        "decoding_cfg = model.cfg.decoding\n",
        "decoding_cfg.beam.beam_size = 1\n",
        "model.change_decoding_strategy(decoding_cfg)\n",
        "\n",
        "# setup for buffered inference\n",
        "model.cfg.preprocessor.dither = 0.0\n",
        "model.cfg.preprocessor.pad_to = 0\n",
        "\n",
        "feature_stride = model.cfg.preprocessor['window_stride']\n",
        "model_stride_in_secs = feature_stride * 8  # 8 = model stride, which is 8 for FastConformer\n",
        "\n",
        "frame_asr = FrameBatchMultiTaskAED(\n",
        "    asr_model=model,\n",
        "    frame_len=40.0,\n",
        "    total_buffer=40.0,\n",
        "    batch_size=16,\n",
        ")\n",
        "\n",
        "amp_dtype = torch.float16"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run Canary 1B\n",
        "def convert_audio(audio_filepath, tmpdir, utt_id):\n",
        "    \"\"\"\n",
        "    Convert all files to monochannel 16 kHz wav files.\n",
        "    Do not convert and raise error if audio too long.\n",
        "    Returns output filename and duration.\n",
        "    \"\"\"\n",
        "\n",
        "    data, sr = librosa.load(audio_filepath, sr=None, mono=True)\n",
        "\n",
        "    duration = librosa.get_duration(y=data, sr=sr)\n",
        "\n",
        "    if sr != SAMPLE_RATE:\n",
        "        data = librosa.resample(data, orig_sr=sr, target_sr=SAMPLE_RATE)\n",
        "\n",
        "    out_filename = os.path.join(tmpdir, utt_id + '.wav')\n",
        "\n",
        "    # save output audio\n",
        "    sf.write(out_filename, data, SAMPLE_RATE)\n",
        "\n",
        "    return out_filename, duration\n",
        "\n",
        "def transcribe(audio_filepath, src_lang, tgt_lang, pnc):\n",
        "\n",
        "    if audio_filepath is None:\n",
        "        raise ValueError(\"Please provide some input audio: either upload an audio file or use the microphone\")\n",
        "\n",
        "    utt_id = uuid.uuid4()\n",
        "    with tempfile.TemporaryDirectory() as tmpdir:\n",
        "        converted_audio_filepath, duration = convert_audio(audio_filepath, tmpdir, str(utt_id))\n",
        "\n",
        "        # map src_lang and tgt_lang from long versions to short\n",
        "        LANG_LONG_TO_LANG_SHORT = {\n",
        "            \"English\": \"en\",\n",
        "            \"Spanish\": \"es\",\n",
        "            \"French\": \"fr\",\n",
        "            \"German\": \"de\",\n",
        "        }\n",
        "        if src_lang not in LANG_LONG_TO_LANG_SHORT.keys():\n",
        "            raise ValueError(f\"src_lang must be one of {LANG_LONG_TO_LANG_SHORT.keys()}\")\n",
        "        else:\n",
        "            src_lang = LANG_LONG_TO_LANG_SHORT[src_lang]\n",
        "\n",
        "        if tgt_lang not in LANG_LONG_TO_LANG_SHORT.keys():\n",
        "            raise ValueError(f\"tgt_lang must be one of {LANG_LONG_TO_LANG_SHORT.keys()}\")\n",
        "        else:\n",
        "            tgt_lang = LANG_LONG_TO_LANG_SHORT[tgt_lang]\n",
        "\n",
        "       # infer taskname from src_lang and tgt_lang\n",
        "        if src_lang == tgt_lang:\n",
        "           taskname = \"asr\"\n",
        "        else:\n",
        "           taskname = \"s2t_translation\"\n",
        "\n",
        "       # update pnc variable to be \"yes\"\n",
        "           pnc = \"yes\" if pnc else \"no\"\n",
        "\n",
        "       # make manifest file and save\n",
        "        manifest_filepath = os.path.join(tmpdir, f'{utt_id}.json')\n",
        "        manifest_data = {\n",
        "           \"audio_filepath\": converted_audio_filepath,\n",
        "           \"source_lang\": src_lang,\n",
        "           \"target_lang\": tgt_lang,\n",
        "           \"taskname\": taskname,\n",
        "           \"pnc\": pnc,\n",
        "           \"answer\": \"predict\",\n",
        "           \"duration\": str(duration),\n",
        "       }\n",
        "        with open(manifest_filepath, 'w') as fout:\n",
        "           line = json.dumps(manifest_data)\n",
        "           fout.write(line + '\\n')\n",
        "\n",
        "       # call transcribe, passing in manifest filepath\n",
        "        if duration < 40:\n",
        "           output_text = model.transcribe(manifest_filepath)[0]\n",
        "        else:  # do buffered inference\n",
        "            with torch.cuda.amp.autocast(dtype=amp_dtype):  # TODO: make it work if no cuda\n",
        "               with torch.no_grad():\n",
        "                   hyps = [get_buffered_pred_feat_multitaskAED(\n",
        "                       frame_asr,\n",
        "                       model.cfg.preprocessor,\n",
        "                       model_stride_in_secs,\n",
        "                       model.device,\n",
        "                       manifest=manifest_filepath,\n",
        "                       filepaths=None,\n",
        "                   )]\n",
        "\n",
        "                   output_text = hyps[0].text\n",
        "\n",
        "    clear_output()\n",
        "    return output_text\n",
        "\n",
        "# USAGE\n",
        "\n",
        "#@markdown Upload an audio file in Files, click the 3 dots next to him and click Copy Path and paste it down here.\n",
        "audio_filepath = \"test.wav\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown **WARNING: Either src_lang or tgt_lang, you need to select english atleast in one of them**\n",
        "\n",
        "#@markdown Input audio is spoken in:\n",
        "src_lang = \"English\" #@param [\"English\", \"Spanish\", \"French\", \"German\"]\n",
        "\n",
        "#@markdown Transcribe in language:\n",
        "tgt_lang = \"English\" #@param [\"English\", \"Spanish\", \"French\", \"German\"]\n",
        "\n",
        "#@markdown Punctuation & Capitalization in transcript?\n",
        "pnc = True #@param {type:\"boolean\"}\n",
        "\n",
        "output_text = transcribe(audio_filepath, src_lang, tgt_lang, pnc)\n",
        "\n",
        "print(output_text)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "yvcw976b_skL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNkDqyt10S1MRBZrhB3cMHT",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}